#!/usr/bin/env python
# get growth of top-level folders over time
# expects data in a schema as generated by vsphere-influxdb-import.py
import sys
import argparse
import time
import datetime

from datetime import datetime as dt
from datetime import timedelta as td
from influxdb.influxdb08 import InfluxDBClient
from pyVsphereInflux import InfluxResult08

influx_dsn_default = "influxdb://root:root@localhost:8086/database"
data_spec = {
                '/vmagg_topLevelFolder\..*/': {
                            'storage_used': 'summary.storage.committed',
                            'storage_bytes_factor': 1,
                            'memory_used': 'config.hardware.memoryMB',
                            'memory_bytes_factor': 2**20,
                            'cpus_used': 'config.hardware.numCPU',
                }
            }


def get_raw_data(args):
    # find raw points in database
    client = InfluxDBClient.from_DSN(args.influx_dsn)
    
    # get series to query
    check_series = {}
    for list_spec in data_spec:
        series = InfluxResult08.query(client, "list series %s" % list_spec)
        check_series[list_spec] = []
        for s in series:
            check_series[list_spec].extend(s.fields.values())

    # query the series and record the results according to the data spec 
    # and command line arguments
    data_points = {}

    for list_spec in check_series:
        for series in check_series[list_spec]:
            query = """SELECT 
                        mean("%s") AS "storage_used",
                        mean("%s") AS "memory_used",
                        mean("%s") AS "cpus_used" 
                       FROM "%s"
                       WHERE time > now() - %sd
                       GROUP BY time(%sd) ORDER ASC""" % \
                       (data_spec[list_spec]['storage_used'], 
                        data_spec[list_spec]['memory_used'], 
                        data_spec[list_spec]['cpus_used'], 
                        series, args.range, args.interval)

            res = InfluxResult08.query(client, query)
            for ts in res:
                ts.tags['storage_bytes_factor'] = \
                    data_spec[list_spec]['storage_bytes_factor']
                ts.tags['memory_bytes_factor'] = \
                    data_spec[list_spec]['memory_bytes_factor']
            data_points[series] = res

    return data_points

def main():
    # take some input 
    parser = argparse.ArgumentParser(description="get growth of top level vSphere folders over time")
    parser.add_argument('--influx-dsn', default=influx_dsn_default,
                        help="InfluxDB DSN, eg. %s" % influx_dsn_default)
    parser.add_argument('--range', default=120, type=int,
                        help="Range of history to search in days (default: 120)")
    parser.add_argument('--interval', default=1, type=int,
                        help="Summarization interval for data points in days (default: 1)")
    parser.add_argument('--top', default=10, type=int,
                        help="Report top N results, -1 for all")
    parser.add_argument('--sort', default="storage", 
                         choices=["storage", "memory", "cpus"],
                         help="Sort by storage, memory, or cpus, default=storage")
    parser.add_argument('--metric', default="growth",
                         choices=["first_used", "latest_used", "growth"],
                         help="Sort by storage, memory, or cpus, default=storage")
    parser.add_argument('--debug', '-d', action='store_true', 
                        help="enable debugging")

    args = parser.parse_args()

    data_points = get_raw_data(args)
    results = {}
    now = datetime.datetime.now()
    timerange = td(days=args.range)
    timeinterval = td(days=args.interval)

    for series in data_points:
        if len(data_points[series]) > 0:
            first_ts = data_points[series][0]
            latest_ts = data_points[series][-1]
            
            # factors used to normalize to bytes
            storage_bytes_factor = latest_ts.tags['storage_bytes_factor']
            memory_bytes_factor = latest_ts.tags['memory_bytes_factor']

            # first values
            first_point_time = dt.fromtimestamp(first_ts.timestamp)
            if args.debug:
                print first_point_time + timerange, now, series
            # if the first point plus the range is greater than now,
            # then the first point must be later than usual, meaning
            # the object came into existence after the start of the range
            if first_point_time + timerange > now:
                storage_first_used = 0
                memory_first_used = 0
                cpus_first_used = 0
            else:
                storage_first_used = \
                    first_ts.fields['storage_used'] * storage_bytes_factor
                memory_first_used = \
                    first_ts.fields['memory_used'] * memory_bytes_factor
                cpus_first_used = first_ts.fields['cpus_used']

            # latest values
            latest_point_time = dt.fromtimestamp(latest_ts.timestamp)
            if args.debug:
                print latest_point_time + timeinterval, now, series
            # if the last point minus the summarization interval is less than
            # now, then the last point must have been earlier than usual,
            # meaning that the object left existence before the end of the
            # range
            if latest_point_time + timeinterval < now:
                storage_latest_used = 0
                memory_latest_used = 0
                cpus_latest_used = 0
            else:
                storage_latest_used = \
                    latest_ts.fields['storage_used'] * storage_bytes_factor
                memory_latest_used = \
                    latest_ts.fields['memory_used'] * memory_bytes_factor
                cpus_latest_used = latest_ts.fields['cpus_used']

            # growth
            storage_growth = storage_latest_used - storage_first_used
            memory_growth = memory_latest_used - memory_first_used
            cpus_growth = cpus_latest_used - cpus_first_used

            # stuff the results into a dict
            results[series] = {}
            results[series]['storage_first_used'] = storage_first_used
            results[series]['memory_first_used'] = memory_first_used
            results[series]['cpus_first_used'] = cpus_first_used
            results[series]['storage_latest_used'] = storage_latest_used
            results[series]['memory_latest_used'] = memory_latest_used
            results[series]['cpus_latest_used'] = cpus_latest_used
            results[series]['storage_growth'] = storage_growth
            results[series]['memory_growth'] = memory_growth
            results[series]['cpus_growth'] = cpus_growth

    output_keys = sorted(results, 
                         key=lambda x: results[x]['%s_%s' % \
                             (args.sort, args.metric)],
                         reverse=True)[:args.top]

    if args.debug:
        print "Raw data points:"
        for s in data_points:
            print "Series:", s
            print "Points:", data_points[s]
            print
        print "Growth:"
        for s in results:
            print "Series:", s
            print "Results:", results[s]
            print

    # print results
    print "Top-Level Folder, Storage Growth (GB), Memory Growth (GB), CPU Growth"
    for series in output_keys:
        r_val = results[series]
        series_str = ".".join(series.split(".")[1:])

        print "%s, %.2f, %.2f, %d" % \
            (series_str, 
             r_val['storage_growth'] / (2**30),
             r_val['memory_growth'] / (2**30),
             r_val['cpus_growth'])


if __name__ == '__main__':
    sys.exit(main())

# vim: et:ai:sw=4:ts=4
